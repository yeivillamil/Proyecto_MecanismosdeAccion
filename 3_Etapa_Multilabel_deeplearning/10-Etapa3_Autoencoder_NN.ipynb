{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 ETAPA: Autoencoder usando el conjunto original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta notebook, es entrenado un autoencoder a partir de redes neurales con el conjunto de datos original. \n",
    "\n",
    "Inicialmente, será un entrenado un `autoencoder` y este será usados para el entrenamiento de una arquitectura de red neuronales fija. Para la segunda etapa, será usado este mismo autoencoder pero se usará el wrapper de Keras para poder buscar los mejores hiperparámetros de la red mediante `GridSearchCV`. Los parámetros que serán variados son la cantidad de neuronas en cada capa de la red neuronal.\n",
    "\n",
    "Para el uso de este notebook es necesario instalar los siguientes módulos:\n",
    "* pip install scikit-multilearn\n",
    "* pip install tensorflow\n",
    "* pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit\n",
    "from skmultilearn.model_selection import IterativeStratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loga_loss(y_test, y_pred, eps=1e-15):\n",
    "    \"\"\"función para el cálculo de la perdida logarítmica establecida en el concurso de kaggle\n",
    "    \n",
    "    y_test --> Variable de salida de prueba\n",
    "    \n",
    "    y_pred --> Variable predicha (Predicción de probabilidad de activación)\"\"\"\n",
    "    \n",
    "    los = np.zeros(y_test.shape)\n",
    "    n, m = y_test.shape\n",
    "    y_true = np.clip(y_test, eps, 1-eps)\n",
    "    for M in range(m):\n",
    "        for N in range(n):\n",
    "            log_los = -((y_true[N,M]*np.log(y_pred[N,M]+eps))+((1-y_true[N,M])*np.log(1-y_pred[N,M]+eps)))\n",
    "            los[N,M] = log_los\n",
    "    return los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 10\n",
    "cv_i = IterativeStratification(n_splits=nfolds)\n",
    "cv = StratifiedKFold(n_splits = nfolds, shuffle=False)\n",
    "cv_k = KFold(n_splits = nfolds, shuffle=True)\n",
    "cv_s = StratifiedShuffleSplit(n_splits=nfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=0)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = pd.read_csv('train_features.csv')\n",
    "trainTargetScored = pd.read_csv('train_targets_scored.csv')\n",
    "testFeatures = pd.read_csv('test_features.csv')\n",
    "\n",
    "trainFeatures['cp_time'] = trainFeatures['cp_time'].map({24:1, 48:2, 72:3})\n",
    "trainFeatures['cp_dose'] = trainFeatures['cp_dose'].map({'D1':0, 'D2':1})\n",
    "trainFeatures = trainFeatures.drop(columns=\"sig_id\")\n",
    "trainTargetScored = trainTargetScored.drop(columns=\"sig_id\")\n",
    "\n",
    "testFeatures['cp_time'] = testFeatures['cp_time'].map({24:1, 48:2, 72:3})\n",
    "testFeatures['cp_dose'] = testFeatures['cp_dose'].map({'D1':0, 'D2':1})\n",
    "testFeatures = testFeatures.drop(columns=\"sig_id\")\n",
    "\n",
    "#Seperating gene and cell columns\n",
    "gene_cols = [c for c in trainFeatures.columns if c.startswith('g-')]\n",
    "cell_cols = [c for c in trainFeatures.columns if c.startswith('c-')]\n",
    "#using QunatileTransformer to transform oue gene and cell columns\n",
    "#QunatileTransformer method transforms the features to follow a uniform or a normal distribution.\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "qt = QuantileTransformer(n_quantiles=100, random_state=0)\n",
    "qt.fit(trainFeatures[gene_cols + cell_cols])\n",
    "trainFeatures[gene_cols+cell_cols] = qt.transform(trainFeatures[gene_cols + cell_cols])\n",
    "testFeatures[gene_cols+cell_cols] = qt.transform(testFeatures[gene_cols + cell_cols])\n",
    "\n",
    "DataTrain = pd.concat([trainFeatures, trainTargetScored], axis = 1)\n",
    "Train = DataTrain[DataTrain['cp_type'] == 'trt_cp']\n",
    "Evaluar = testFeatures[testFeatures['cp_type'] == 'trt_cp']\n",
    "\n",
    "\n",
    "trainFeature_X = Train.iloc[:,1:875].reset_index(drop=True)\n",
    "targetsCols_y = Train.iloc[:,876:].reset_index(drop=True)\n",
    "\n",
    "higher = [col for col in targetsCols_y if (targetsCols_y[col].sum() > 100)]\n",
    "Targety_H = targetsCols_y[higher]\n",
    "\n",
    "featuresCount = trainFeature_X.shape[1]\n",
    "print(\"Features count = %d\" % featuresCount)\n",
    "\n",
    "targetsCols  = Targety_H.columns\n",
    "targetsCount = len(targetsCols)\n",
    "print(\"Targets count = %d\" % targetsCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split iterativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = iterative_train_test_split(np.array(trainFeature_X), np.array(Targety_H), test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder Model Preparation\n",
    "\n",
    "def autoencoder():\n",
    "    n_inputs = trainFeature_X.shape[1]\n",
    "    # define encoder\n",
    "    input_data_shape= Input(shape=(n_inputs,))\n",
    "    # encoder level\n",
    "    encoder= Dense(512, activation='relu')(input_data_shape)\n",
    "    encoder= Dense(128, activation='relu')(encoder)\n",
    "    encoder= Dense(64, activation='relu')(encoder)\n",
    "    encoder= Dense(32, activation='relu')(encoder)\n",
    "    # bottleneck\n",
    "    n_bottleneck = 50\n",
    "    bottleneck = Dense(n_bottleneck)(encoder)\n",
    "    # define decoder\n",
    "    decoder = Dense(32, activation='relu')(bottleneck)\n",
    "    decoder = Dense(64, activation='relu')(decoder)\n",
    "    decoder = Dense(128, activation='relu')(decoder)\n",
    "    decoder = Dense(512, activation='relu')(decoder)\n",
    "    # output layer\n",
    "    output = Dense(n_inputs, activation='linear')(decoder)\n",
    "    # define autoencoder model\n",
    "    autoencoder = tf.keras.Model(inputs=input_data_shape, outputs=output)\n",
    "    # compile autoencoder model\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = autoencoder()\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train, X_train, epochs=250, callbacks=[reduce_lr, early_stop], batch_size=128, shuffle=True, \n",
    "                validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Red neuronal incluyendo el autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_NN(autoencoder):\n",
    "    for layer in autoencoder.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    input_1 = autoencoder.input\n",
    "    output = autoencoder.layers[5].output\n",
    "    layer_input = L.BatchNormalization()(input_1)\n",
    "    \n",
    "    layer_1 = tfa.layers.WeightNormalization(L.Dense(874))(output)\n",
    "    layer_1 = L.ReLU()(layer_1)\n",
    "    layer_1 = L.BatchNormalization()(layer_1)\n",
    "    layer_1 = L.Dropout(0.2)(layer_1)\n",
    "\n",
    "    layer_2 = tfa.layers.WeightNormalization(L.Dense(500))(layer_1)\n",
    "    layer_2 = L.ReLU()(layer_2)\n",
    "    layer_2 = L.BatchNormalization()(layer_2)\n",
    "    layer_2 = L.Dropout(0.2)(layer_2)\n",
    "\n",
    "    layer_3 = tfa.layers.WeightNormalization(L.Dense(250))(layer_2)\n",
    "    layer_3 = L.ReLU()(layer_3)\n",
    "    layer_3 = L.BatchNormalization()(layer_3)\n",
    "    layer_3 = L.Dropout(0.2)(layer_3)\n",
    "\n",
    "    layer_4 = tfa.layers.WeightNormalization(L.Dense(100))(layer_3)\n",
    "    layer_4 = L.ReLU()(layer_4)\n",
    "    layer_4 = L.BatchNormalization()(layer_4)\n",
    "    layer_4 = L.Dropout(0.2)(layer_4)\n",
    "\n",
    "    classifier = tfa.layers.WeightNormalization(L.Dense(41, activation=\"sigmoid\"))(layer_4)\n",
    "\n",
    "    model = tf.keras.Model(input_1, classifier)\n",
    "\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(lr=0.001, weight_decay=1e-5 , clipvalue=900), loss='binary_crossentropy')\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "modelo_nn, log_nn = [],[]\n",
    "for (fn, (train_ind, val_ind)) in enumerate(cv_s.split(X_train, y_train)):\n",
    "    X_tr, X_val = np.array(X_train[train_ind]), np.array(X_train[val_ind])\n",
    "    y_tr, y_val = np.array(y_train[train_ind]), np.array(y_train[val_ind])\n",
    "    print(f'fold {fn+1}')        \n",
    "    #Cheking empty columns\n",
    "    check_for_empty_cols = np.where(y_tr.sum(axis = 0) == 0)[0]\n",
    "    if len(check_for_empty_cols):\n",
    "                y_tr[0,check_for_empty_cols] = 1\n",
    "    \n",
    "    modelo = model_NN(autoencoder)\n",
    "\n",
    "    #Training\n",
    "    modelo.fit(X_tr, y_tr, validation_data=(X_val, y_val), callbacks=[reduce_lr, early_stop], epochs=100, batch_size=128)\n",
    "    score = modelo.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # y_pred = modelo.predict(X_val)\n",
    "    \n",
    "    # log_v = loga_loss(y_val, y_pred, eps=1e-15)\n",
    "    print(f'fold {fn+1} --> log_loss: {round(score,5)}')\n",
    "    print('--------------------------')\n",
    "    modelo_nn.append(modelo)\n",
    "    log_nn.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_NN():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(L.Input(train_features.shape[1]))\n",
    "    model.add(L.BatchNormalization())\n",
    "    \n",
    "    model.add(tfa.layers.WeightNormalization(L.Dense(874)))\n",
    "    model.add(L.ReLU())\n",
    "    model.add(L.BatchNormalization())\n",
    "    model.add(L.Dropout(0.2))\n",
    "\n",
    "    model.add(tfa.layers.WeightNormalization(L.Dense(500)))\n",
    "    model.add(L.ReLU())\n",
    "    model.add(L.BatchNormalization())\n",
    "    model.add(L.Dropout(0.2))\n",
    "\n",
    "    model.add(tfa.layers.WeightNormalization(L.Dense(250)))\n",
    "    model.add(L.ReLU())\n",
    "    model.add(L.BatchNormalization())\n",
    "    model.add(L.Dropout(0.2))\n",
    "    \n",
    "    model.add(tfa.layers.WeightNormalization(L.Dense(100)))\n",
    "    model.add(L.ReLU())\n",
    "    model.add(L.BatchNormalization())\n",
    "    model.add(L.Dropout(0.2))\n",
    "\n",
    "    model.add(tfa.layers.WeightNormalization(L.Dense(41, activation=\"sigmoid\")))\n",
    "\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(lr=0.001, weight_decay=1e-5 , clipvalue=900), loss='binary_crossentropy')\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_nn, log_nn = [],[]\n",
    "for (fn, (train_ind, val_ind)) in enumerate(cv_s.split(train_features, y_train)):\n",
    "    X_tr, X_val = np.array(train_features[train_ind]), np.array(train_features[val_ind])\n",
    "    y_tr, y_val = np.array(y_train[train_ind]), np.array(y_train[val_ind])\n",
    "    print(f'fold {fn+1}')        \n",
    "    #Cheking empty columns\n",
    "    check_for_empty_cols = np.where(y_tr.sum(axis = 0) == 0)[0]\n",
    "    if len(check_for_empty_cols):\n",
    "                y_tr[0,check_for_empty_cols] = 1\n",
    "    \n",
    "    modelo = model_NN()\n",
    "\n",
    "    #Training\n",
    "    modelo.fit(X_tr, y_tr, validation_data=(X_val, y_val), callbacks=[reduce_lr, early_stop], epochs=100, batch_size=128)\n",
    "    score = modelo.evaluate(test_features, y_test, verbose=0)\n",
    "\n",
    "    # y_pred = modelo.predict(X_val)\n",
    "    \n",
    "    # log_v = loga_loss(y_val, y_pred, eps=1e-15)\n",
    "    print(f'fold {fn+1} --> log_loss: {round(score,5)}')\n",
    "    print('--------------------------')\n",
    "    modelo_nn.append(modelo)\n",
    "    log_nn.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(modelo_nn)):\n",
    "    pred = modelo_nn[i].predict(X_test)\n",
    "    l_loss = loga_loss(y_test, pred, eps=1e-15)\n",
    "    print(f'Cross-entropy score for model {i+1}')\n",
    "    print(round(np.mean(l_loss),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Arquitectura de red neural usango GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr_2 = ReduceLROnPlateau(monitor='loss', patience=3, verbose=0)\n",
    "early_stop_2 = EarlyStopping(monitor='loss', patience=6, restore_best_weights=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MODEL_NN(l_1, l_2, l_3, l_4):\n",
    "    for layer in autoencoder.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    input_1 = autoencoder.input\n",
    "    output = autoencoder.layers[5].output\n",
    "    layer_input = L.BatchNormalization()(input_1)\n",
    "    \n",
    "    layer_1 = tfa.layers.WeightNormalization(L.Dense(l_1))(output)\n",
    "    layer_1 = L.ReLU()(layer_1)\n",
    "    layer_1 = L.BatchNormalization()(layer_1)\n",
    "    layer_1 = L.Dropout(0.2)(layer_1)\n",
    "\n",
    "    layer_2 = tfa.layers.WeightNormalization(L.Dense(l_2))(layer_1)\n",
    "    layer_2 = L.ReLU()(layer_2)\n",
    "    layer_2 = L.BatchNormalization()(layer_2)\n",
    "    layer_2 = L.Dropout(0.2)(layer_2)\n",
    "\n",
    "    layer_3 = tfa.layers.WeightNormalization(L.Dense(l_3))(layer_2)\n",
    "    layer_3 = L.ReLU()(layer_3)\n",
    "    layer_3 = L.BatchNormalization()(layer_3)\n",
    "    layer_3 = L.Dropout(0.2)(layer_3)\n",
    "\n",
    "    layer_4 = tfa.layers.WeightNormalization(L.Dense(l_4))(layer_3)\n",
    "    layer_4 = L.ReLU()(layer_4)\n",
    "    layer_4 = L.BatchNormalization()(layer_4)\n",
    "    layer_4 = L.Dropout(0.2)(layer_4)\n",
    "\n",
    "    classifier = tfa.layers.WeightNormalization(L.Dense(41, activation=\"sigmoid\"))(layer_4)\n",
    "\n",
    "    model = tf.keras.Model(input_1, classifier)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=['adam','rmsprop'] \n",
    "l_1 = [700, 874, 1000]\n",
    "l_2 = [350, 450, 500]\n",
    "l_3 = [150, 250, 300]\n",
    "l_4 = [50, 75, 100]\n",
    "param_grid = dict(l_1=l_1, l_2=l_2, l_3=l_3, l_4=l_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = model_NN(autoencoder, l_1=874, l_2=500, l_3=250, l_4=100, optimizer='adam')\n",
    "modelo = KerasClassifier(build_fn=MODEL_NN, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 4\n",
    "cv_s = StratifiedShuffleSplit(n_splits=nfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=modelo, param_grid=param_grid, cv=cv_s)#, scoring='accuracy')\n",
    "grid.fit(X_train, y_train, callbacks=[reduce_lr_2, early_stop_2])##,epochs=100, batch_size=128, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_NN(autoencoder):\n",
    "    for layer in autoencoder.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    input_1 = autoencoder.input\n",
    "    output = autoencoder.layers[5].output\n",
    "    layer_input = L.BatchNormalization()(input_1)\n",
    "    \n",
    "    layer_1 = tfa.layers.WeightNormalization(L.Dense(874))(output)\n",
    "    layer_1 = L.ReLU()(layer_1)\n",
    "    layer_1 = L.BatchNormalization()(layer_1)\n",
    "    layer_1 = L.Dropout(0.2)(layer_1)\n",
    "\n",
    "    layer_2 = tfa.layers.WeightNormalization(L.Dense(500))(layer_1)\n",
    "    layer_2 = L.ReLU()(layer_2)\n",
    "    layer_2 = L.BatchNormalization()(layer_2)\n",
    "    layer_2 = L.Dropout(0.2)(layer_2)\n",
    "\n",
    "    layer_3 = tfa.layers.WeightNormalization(L.Dense(200))(layer_2)\n",
    "    layer_3 = L.ReLU()(layer_3)\n",
    "    layer_3 = L.BatchNormalization()(layer_3)\n",
    "    layer_3 = L.Dropout(0.2)(layer_3)\n",
    "\n",
    "    layer_4 = tfa.layers.WeightNormalization(L.Dense(75))(layer_3)\n",
    "    layer_4 = L.ReLU()(layer_4)\n",
    "    layer_4 = L.BatchNormalization()(layer_4)\n",
    "    layer_4 = L.Dropout(0.2)(layer_4)\n",
    "\n",
    "    classifier = tfa.layers.WeightNormalization(L.Dense(41, activation=\"sigmoid\"))(layer_4)\n",
    "\n",
    "    model = tf.keras.Model(input_1, classifier)\n",
    "\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(lr=0.001, weight_decay=1e-5 , clipvalue=900), loss='binary_crossentropy')\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_nn, log_nn = [],[]\n",
    "for (fn, (train_ind, val_ind)) in enumerate(cv_s.split(X_train, y_train)):\n",
    "    X_tr, X_val = np.array(X_train[train_ind]), np.array(X_train[val_ind])\n",
    "    y_tr, y_val = np.array(y_train[train_ind]), np.array(y_train[val_ind])\n",
    "    print(f'fold {fn+1}')        \n",
    "    #Cheking empty columns\n",
    "    check_for_empty_cols = np.where(y_tr.sum(axis = 0) == 0)[0]\n",
    "    if len(check_for_empty_cols):\n",
    "                y_tr[0,check_for_empty_cols] = 1\n",
    "    \n",
    "    modelo = model_NN(autoencoder)\n",
    "\n",
    "    #Training\n",
    "    modelo.fit(X_tr, y_tr, validation_data=(X_val, y_val), callbacks=[reduce_lr, early_stop], epochs=100, batch_size=128)\n",
    "    score = modelo.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # y_pred = modelo.predict(X_val)\n",
    "    \n",
    "    # log_v = loga_loss(y_val, y_pred, eps=1e-15)\n",
    "    print(f'fold {fn+1} --> log_loss: {round(score,5)}')\n",
    "    print('--------------------------')\n",
    "    modelo_nn.append(modelo)\n",
    "    log_nn.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_nn, log_nn = [],[]\n",
    "for (fn, (train_ind, val_ind)) in enumerate(cv_s.split(X_train, y_train)):\n",
    "    X_tr, X_val = np.array(X_train[train_ind]), np.array(X_train[val_ind])\n",
    "    y_tr, y_val = np.array(y_train[train_ind]), np.array(y_train[val_ind])\n",
    "    print(f'fold {fn+1}')        \n",
    "    #Cheking empty columns\n",
    "    check_for_empty_cols = np.where(y_tr.sum(axis = 0) == 0)[0]\n",
    "    if len(check_for_empty_cols):\n",
    "                y_tr[0,check_for_empty_cols] = 1\n",
    "    \n",
    "    modelo = model_NN(autoencoder)\n",
    "\n",
    "    #Training\n",
    "    modelo.fit(X_tr, y_tr, validation_data=(X_val, y_val), callbacks=[reduce_lr, early_stop], epochs=100, batch_size=128)\n",
    "    score = modelo.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # y_pred = modelo.predict(X_val)\n",
    "    \n",
    "    # log_v = loga_loss(y_val, y_pred, eps=1e-15)\n",
    "    print(f'fold {fn+1} --> log_loss: {round(score,5)}')\n",
    "    print('--------------------------')\n",
    "    modelo_nn.append(modelo)\n",
    "    log_nn.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred =  modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN=loga_loss(y_test, ypred, eps=1e-15)\n",
    "print(f'log_loss: {round(np.mean(NN),3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
