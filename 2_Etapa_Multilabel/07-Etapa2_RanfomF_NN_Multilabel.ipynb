{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 ETAPA: Random Forest y Redes Neurales: Problema multietiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta sección es el entrenamiento de modelos basados en arboles de decisión y redes neuronales (usando tanto la librería scikit-learn como tensorflow) para abordar el problema multietiqueta.\n",
    "\n",
    "Para la ejecución de este notebook, es importante la instalación de las siguientes módulos:\n",
    "\n",
    "* pip install scikit-multilearn\n",
    "* pip install imblearn\n",
    "* pip install tensorflow\n",
    "\n",
    "En este notebook se encuentra la ejecución de los modelos random forest y redes neurales, tomandolos de las librerías de scikit-learn. Para cada modelos, se realiza un cross-validation manual y se obtiene el mejor modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, plot_confusion_matrix, confusion_matrix, log_loss, recall_score, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "import seaborn as sn\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.model_selection import IterativeStratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('train_features.csv')\n",
    "train_target = pd.read_csv('train_targets_scored.csv')\n",
    "test_features = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding cp_type, cp_time and cp_dose categorical columns of train dataset\n",
    "# train_features_['cp_type'] = train_features_['cp_type'].map({'trt_cp':0, 'ctl_vehicle':1})\n",
    "train_features['cp_time'] = train_features['cp_time'].map({24:1, 48:2, 72:3})\n",
    "train_features['cp_dose'] = train_features['cp_dose'].map({'D1':0, 'D2':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat([train_features, train_target], axis = 1)\n",
    "train = data_train[data_train['cp_type'] == 'trt_cp']\n",
    "evaluar = test_features[test_features['cp_type'] == 'trt_cp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de datos\n",
    "X = train.iloc[:,2:876].reset_index(drop=True)\n",
    "y = train.iloc[:,877:].reset_index(drop=True)\n",
    "x_eval = evaluar.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher = [col for col in y.columns if (y[col].sum() > 100)]\n",
    "ytarget = y[higher]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split iterativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = iterative_train_test_split(np.array(X), np.array(ytarget), test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 5\n",
    "cv_i = IterativeStratification(n_splits=nfolds)\n",
    "cv = StratifiedKFold(n_splits = nfolds, shuffle=False)\n",
    "cv_k = KFold(n_splits = nfolds)\n",
    "cv_s = StratifiedShuffleSplit(n_splits=nfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loga_loss(y_test, y_pred, eps=1e-15):\n",
    "    \"\"\"función para el cálculo de la perdida logarítmica establecida en el concurso de kaggle\n",
    "    \n",
    "    y_test --> Variable de salida de prueba\n",
    "    \n",
    "    y_pred --> Variable predicha (Predicción de probabilidad de activación)\"\"\"\n",
    "    \n",
    "    los = np.zeros(y_test.shape)\n",
    "    n, m = y_test.shape\n",
    "    y_true = np.clip(y_test, eps, 1-eps)\n",
    "    for M in range(m):\n",
    "        for N in range(n):\n",
    "            log_los = -((y_true[N,M]*np.log(y_pred[N,M]+eps))+((1-y_true[N,M])*np.log(1-y_pred[N,M]+eps)))\n",
    "            los[N,M] = log_los\n",
    "    return los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(xtrain, ytrain, xtest, ytest, model):\n",
    "    \"\"\"función para el entrenamiento y prueba de los modelos generados en el la sección de cross_validation. \n",
    "    EL objetivo es reentrenar con los datos de pruebar y verificar las métricas obtenidas en el paso anterior de CV\n",
    "    \n",
    "    xtrain --> Dataset de entrenamiento\n",
    "    \n",
    "    ytrain --> Variables objetivo de entrenamiento\n",
    "    \n",
    "    xtest --> Dataset de prueba\n",
    "    \n",
    "    ytest --> Variables objetivo de prueba\n",
    "    \n",
    "    model --> Lista con los modelos generados en el paso de cross_validation\"\"\"\n",
    "    \n",
    "    for i in range(len(model)):\n",
    "        \n",
    "        #Training\n",
    "        train = model[i].fit(xtrain, ytrain)\n",
    "        \n",
    "        #Predictions and metrics\n",
    "        pred = train.predict_proba(xtest)\n",
    "        log_l = np.array(pred)[:,:,1].T\n",
    "        \n",
    "        log_loss = loga_loss(ytest, pred, eps=1e-15)\n",
    "        print(f'Log_loss for model {i+1} --> {round(np.mean(log_loss),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(xtest, ytest, model):\n",
    "    \"\"\"función para la prueba de los modelos generados en el la sección de cross_validation. EL objetivo es predecir con \n",
    "    los datos de prueba y verificar las métricas obtenidas en el paso anterior de CV\n",
    "    \n",
    "    xtest --> Dataset de prueba\n",
    "    \n",
    "    ytest --> Variables objetivo de prueba\n",
    "    \n",
    "    model --> Lista con los modelos generados en el paso de cross_validation\"\"\"\n",
    "    \n",
    "    for i in range(len(model)):\n",
    "              \n",
    "        #Predictions and metrics\n",
    "        pred = model[i].predict_proba(xtest)\n",
    "        log_l = np.array(pred)[:,:,1].T\n",
    "        \n",
    "        log_loss = loga_loss(ytest, log_l, eps=1e-15)\n",
    "        # log_loss = loga_loss(ytest, pred, eps=1e-15)\n",
    "        print(f'Log_loss for model {i+1} --> {round(np.mean(log_loss),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_multiLabel_confusion_matrix(y_test_g,y_est_g):\n",
    "    n_samples, n_class = y_test_g.shape\n",
    "    CM = np.zeros((n_class,n_class))\n",
    "    Temp = np.zeros((1,n_class))\n",
    "    def acum_CM(y_test,y_est,CM,Temp):\n",
    "        ind_real = np.asarray(y_test > 0).nonzero()[0]\n",
    "        ind_est = np.asarray(y_est > 0).nonzero()[0]\n",
    "        #--------------------------------\n",
    "        if ind_real.size == 0:\n",
    "            #In case in the ground truth not even one class is active\n",
    "            Temp = Temp + y_est\n",
    "        elif ind_est.size == 0:\n",
    "            return CM, Temp\n",
    "        else:\n",
    "            mesh_real = np.array(np.meshgrid(ind_real,ind_real))\n",
    "            comb_real = mesh_real.T.reshape(-1, 2)\n",
    "            ind_remove_real = comb_real[:,0] != comb_real[:,1]\n",
    "            comb_real = comb_real[ind_remove_real]\n",
    "            #--------------------------------\n",
    "            mesh_est = np.array(np.meshgrid(ind_real,ind_est))\n",
    "            comb_est = mesh_est.T.reshape(-1, 2)\n",
    "            #--------------------------------\n",
    "            comb_real2 = comb_real[:,0] + comb_real[:,1]*1j\n",
    "            comb_est2 = comb_est[:,0] + comb_est[:,1]*1j\n",
    "            ind_remove = np.in1d(comb_est2,comb_real2)\n",
    "            comb_est = comb_est[np.logical_not(ind_remove)]\n",
    "            #--------------------------------\n",
    "            CM[comb_est[:,0],comb_est[:,1]] += 1\n",
    "        return CM, Temp\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        CM,Temp = acum_CM(y_test_g[i,:],y_est_g[i,:],CM,Temp)\n",
    "        \n",
    "    return CM,Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters_rf = {'n_estimators':[20,40,60,80,100],'max_depth':[2,4,6,8,10,12]}\n",
    "values = [parameters_rf[k] for k in parameters_rf]\n",
    "modelo_rf, ml_rf, log_rf = [],[],[]\n",
    "for i in values[0]:\n",
    "    for j in values[1]:\n",
    "        print('PARAMETER COMBINATION')\n",
    "        print('n_estimator, max_depth:', (i, j))\n",
    "        clf_rf = RandomForestClassifier(criterion='entropy', class_weight='balanced', \n",
    "                                                              n_estimators=i, max_depth=j, n_jobs=-1)\n",
    "        scaler = RobustScaler()\n",
    "        clf_pipe = Pipeline(steps=[('scaler', scaler), ('clf_rf', clf_rf)])\n",
    "        for (fn, (train_ind, val_ind)) in enumerate(cv_s.split(X_train, y_train)):\n",
    "            X_tr, X_val = np.array(X_train[train_ind]), np.array(X_train[val_ind])\n",
    "            y_tr, y_val = np.array(y_train[train_ind]), np.array(y_train[val_ind])\n",
    "            \n",
    "            #Cheking empty columns\n",
    "            check_for_empty_cols = np.where(y_tr.sum(axis = 0) == 0)[0]\n",
    "            if len(check_for_empty_cols):\n",
    "                y_tr[0,check_for_empty_cols] = 1\n",
    "            \n",
    "            #Training\n",
    "            clf_pipe.fit(X_tr, y_tr)\n",
    "    \n",
    "            y_pred = clf_pipe.predict_proba(X_val)\n",
    "            log_val = np.array(y_pred)[:,:,1].T\n",
    "    \n",
    "            log_v = loga_loss(y_val, log_val, eps=1e-15)\n",
    "            print(f'fold {fn+1} --> log_loss: {round(np.mean(log_v),3)}')\n",
    "            print('--------------------------')\n",
    "            ml_rf.append(clf_pipe)\n",
    "            log_rf.append(np.mean(log_v))\n",
    "        l = 0\n",
    "        for k in range(len(log_rf)-1):\n",
    "            if log_rf[k] <= log_rf[k+1]:\n",
    "                l = k\n",
    "        modelo_rf.append(ml_rf[l])\n",
    "        print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Prueba de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(X_test, y_test, modelo_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-layer Perceptron classifier (Neural network sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters_mlp = {'activation':['logistic', 'tanh', 'relu'],'hidden_layer_sizes':[900,1000,1500,1700,2000],\n",
    "            'learning_rate_init':[0.001,0.003,0.005]}\n",
    "values_mlp = [parameters_mlp[k] for k in parameters_mlp]\n",
    "modelo_mlp, ml_mlp, log_mlp = [],[],[]\n",
    "for i in values_mlp[0]:\n",
    "    for j in values_mlp[1]:\n",
    "        for l in values_mlp[2]:\n",
    "            print('PARAM_G COMB')\n",
    "            print('activation, layer, learn_rate:', (i, j, l))\n",
    "            clf_mlp = MLPClassifier(learning_rate='adaptive', random_state=42, activation=i,\n",
    "                        hidden_layer_sizes=(j,y_train.shape[1]), learning_rate_init=l, max_iter=500)\n",
    "            scaler = RobustScaler()\n",
    "            mlp_pipe = Pipeline(steps=[('scaler', scaler), ('clf_rf', clf_mlp)])\n",
    "            for (fn, (train_ind, val_ind)) in enumerate(cv_s.split(X_train, y_train)):\n",
    "                X_tr, X_val = np.array(X_train[train_ind]), np.array(X_train[val_ind])\n",
    "                y_tr, y_val = np.array(y_train[train_ind]), np.array(y_train[val_ind])\n",
    "            \n",
    "                #Cheking empty columns\n",
    "                check_for_empty_cols = np.where(y_tr.sum(axis = 0) == 0)[0]\n",
    "                if len(check_for_empty_cols):\n",
    "                    y_tr[0,check_for_empty_cols] = 1\n",
    "            \n",
    "                #Training\n",
    "                mlp_pipe.fit(X_tr, y_tr)\n",
    "    \n",
    "                y_pred = mlp_pipe.predict_proba(X_val)\n",
    "                # log_val = np.array(y_pred)[:,:,1].T\n",
    "    \n",
    "                log_v = loga_loss(y_val, y_pred, eps=1e-15)\n",
    "                print(f'fold {fn+1} --> log_loss: {round(np.mean(log_v),3)}')\n",
    "                print('--------------------------')\n",
    "                ml_mlp.append(mlp_pipe)\n",
    "                log_mlp.append(np.mean(log_v))\n",
    "            p = 0\n",
    "            for k in range(len(log_mlp)-1):\n",
    "                if log_mlp[k] <= log_mlp[k+1]:\n",
    "                    p = k\n",
    "            modelo_mlp.append(ml_mlp[p])\n",
    "            print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prueba de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(X_test, y_test, modelo_mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
