{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 ETAPA: Redes neuronales - tensorflow: Problema multietiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se encuentra el entrenamiento de redes neuronales usando las librerías de tensorflow, la cual debe ser instalada: `pip install tensorflow`.\n",
    "\n",
    "Para este paso, inicialmente fue construida una arquitectura de redes la cual fue entrenada con los conjuntos de entrenamiento y prueba obtenidos del split iterativo. Para un segundo proceso, se usa un wrapper de tensorflow para la aplicación de `GridSearchCV` con el objetivo de encontrar los mejores hiperparámetros y aplicar validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, plot_confusion_matrix, confusion_matrix, log_loss, recall_score, precision_score\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "import seaborn as sn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import log_loss\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import tensorflow_addons as tfa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loga_loss(y_test, y_pred, eps=1e-15):\n",
    "    \"\"\"función para el cálculo de la perdida logarítmica establecida en el concurso de kaggle\n",
    "    \n",
    "    y_test --> Variable de salida de prueba\n",
    "    \n",
    "    y_pred --> Variable predicha (Predicción de probabilidad de activación)\"\"\"\n",
    "    \n",
    "    los = np.zeros(y_test.shape)\n",
    "    n, m = y_test.shape\n",
    "    y_true = np.clip(y_test, eps, 1-eps)\n",
    "    for M in range(m):\n",
    "        for N in range(n):\n",
    "            log_los = -((y_true[N,M]*np.log(y_pred[N,M]+eps))+((1-y_true[N,M])*np.log(1-y_pred[N,M]+eps)))\n",
    "            los[N,M] = log_los\n",
    "    return los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 10\n",
    "cv_i = IterativeStratification(n_splits=nfolds)\n",
    "cv = StratifiedKFold(n_splits = nfolds, shuffle=False)\n",
    "cv_k = KFold(n_splits = nfolds, shuffle=True)\n",
    "cv_s = StratifiedShuffleSplit(n_splits=nfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = pd.read_csv('train_features.csv')\n",
    "trainTargetScored = pd.read_csv('train_targets_scored.csv')\n",
    "testFeatures = pd.read_csv('test_features.csv')\n",
    "\n",
    "trainFeatures['cp_time'] = trainFeatures['cp_time'].map({24:1, 48:2, 72:3})\n",
    "trainFeatures['cp_dose'] = trainFeatures['cp_dose'].map({'D1':0, 'D2':1})\n",
    "trainFeatures = trainFeatures.drop(columns=\"sig_id\")\n",
    "trainTargetScored = trainTargetScored.drop(columns=\"sig_id\")\n",
    "\n",
    "testFeatures['cp_time'] = testFeatures['cp_time'].map({24:1, 48:2, 72:3})\n",
    "testFeatures['cp_dose'] = testFeatures['cp_dose'].map({'D1':0, 'D2':1})\n",
    "testFeatures = testFeatures.drop(columns=\"sig_id\")\n",
    "\n",
    "#Seperating gene and cell columns\n",
    "gene_cols = [c for c in trainFeatures.columns if c.startswith('g-')]\n",
    "cell_cols = [c for c in trainFeatures.columns if c.startswith('c-')]\n",
    "#using QunatileTransformer to transform oue gene and cell columns\n",
    "#QunatileTransformer method transforms the features to follow a uniform or a normal distribution.\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "qt = QuantileTransformer(n_quantiles=100, random_state=0)\n",
    "qt.fit(trainFeatures[gene_cols + cell_cols])\n",
    "trainFeatures[gene_cols+cell_cols] = qt.transform(trainFeatures[gene_cols + cell_cols])\n",
    "testFeatures[gene_cols+cell_cols] = qt.transform(testFeatures[gene_cols + cell_cols])\n",
    "\n",
    "DataTrain = pd.concat([trainFeatures, trainTargetScored], axis = 1)\n",
    "Train = DataTrain[DataTrain['cp_type'] == 'trt_cp']\n",
    "Evaluar = testFeatures[testFeatures['cp_type'] == 'trt_cp']\n",
    "\n",
    "\n",
    "trainFeature_X = Train.iloc[:,1:875].reset_index(drop=True)\n",
    "targetsCols_y = Train.iloc[:,876:].reset_index(drop=True)\n",
    "\n",
    "higher = [col for col in targetsCols_y if (targetsCols_y[col].sum() > 100)]\n",
    "Targety_H = targetsCols_y[higher]\n",
    "\n",
    "featuresCount = trainFeature_X.shape[1]\n",
    "print(\"Features count = %d\" % featuresCount)\n",
    "\n",
    "targetsCols  = Targety_H.columns\n",
    "targetsCount = len(targetsCols)\n",
    "print(\"Targets count = %d\" % targetsCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split iterativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = iterative_train_test_split(np.array(trainFeature_X), np.array(Targety_H), test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Redes Neuronales (arquitectura simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs =X_train.shape[1]\n",
    "outputs = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Input(n_inputs))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=16, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN=loga_loss(y_test, ypred, eps=1e-15)\n",
    "print(f'log_loss: {round(np.mean(NN),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redes neurales: Arquitectura usando GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr_2 = ReduceLROnPlateau(monitor='loss', patience=3, verbose=0)\n",
    "early_stop_2 = EarlyStopping(monitor='loss', patience=6, restore_best_weights=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "def get_model(l_1,l_2,l_3):\n",
    "    model = Sequential()\n",
    "    model.add(Input(874))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(l_1, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(l_2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(l_3, activation='relu'))\n",
    "    model.add(Dense(41, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_1 = [512, 600, 800]\n",
    "l_2 = [200, 256, 312]\n",
    "l_3 = [50, 75, 100]\n",
    "param_grid = dict(l_1=l_1, l_2=l_2, l_3=l_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = KerasClassifier(build_fn=get_model, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 4\n",
    "cv_s = StratifiedShuffleSplit(n_splits=nfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=modelo, param_grid=param_grid, cv=cv_s)#, scoring='accuracy')\n",
    "grid.fit(X_train, y_train, callbacks=[reduce_lr_2, early_stop_2])##,epochs=100, batch_size=128, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(874))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(312, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(41, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_nn, log_nn = [],[]\n",
    "for (fn, (train_ind, val_ind)) in enumerate(cv_s.split(X_train, y_train)):\n",
    "    X_tr, X_val = np.array(X_train[train_ind]), np.array(X_train[val_ind])\n",
    "    y_tr, y_val = np.array(y_train[train_ind]), np.array(y_train[val_ind])\n",
    "    print(f'fold {fn+1}')        \n",
    "    #Cheking empty columns\n",
    "    check_for_empty_cols = np.where(y_tr.sum(axis = 0) == 0)[0]\n",
    "    if len(check_for_empty_cols):\n",
    "                y_tr[0,check_for_empty_cols] = 1\n",
    "    \n",
    "    modelo = get_model()\n",
    "\n",
    "    #Training\n",
    "    modelo.fit(X_tr, y_tr, validation_data=(X_val, y_val), callbacks=[reduce_lr, early_stop], epochs=100, batch_size=128)\n",
    "    score = modelo.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # y_pred = modelo.predict(X_val)\n",
    "    \n",
    "    # log_v = loga_loss(y_val, y_pred, eps=1e-15)\n",
    "    print(f'fold {fn+1} --> log_loss: {score}')\n",
    "    print('--------------------------')\n",
    "    modelo_nn.append(modelo)\n",
    "    log_nn.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelo_nn, log_nn = [],[]\n",
    "for (fn, (train_ind, val_ind)) in enumerate(cv_s.split(X_train, y_train)):\n",
    "    X_tr, X_val = np.array(X_train[train_ind]), np.array(X_train[val_ind])\n",
    "    y_tr, y_val = np.array(y_train[train_ind]), np.array(y_train[val_ind])\n",
    "    print(f'fold {fn+1}')        \n",
    "    #Cheking empty columns\n",
    "    check_for_empty_cols = np.where(y_tr.sum(axis = 0) == 0)[0]\n",
    "    if len(check_for_empty_cols):\n",
    "                y_tr[0,check_for_empty_cols] = 1\n",
    "    \n",
    "    modelo = get_model()\n",
    "\n",
    "    #Training\n",
    "    modelo.fit(X_tr, y_tr, validation_data=(X_val, y_val), callbacks=[reduce_lr, early_stop], epochs=100, batch_size=128)\n",
    "    score = modelo.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "\n",
    "    # y_pred = modelo.predict(X_val)\n",
    "    \n",
    "    # log_v = loga_loss(y_val, y_pred, eps=1e-15)\n",
    "    print(f'fold {fn+1} --> log_loss: {round(score,5)}')\n",
    "    print('--------------------------')\n",
    "    modelo_nn.append(modelo)\n",
    "    log_nn.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred =  modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN=loga_loss(y_test, ypred, eps=1e-15)\n",
    "print(f'log_loss: {round(np.mean(NN),3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
